{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kadesha Ambris\n",
    "Student ID: 816006483"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from operator import multiply, add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the following sentence to a python tuple list of letters and their frequency as their\n",
    "appear. Ignore all non-alpha numeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog and the fox was very happy\"\n",
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thequickbrownfoxjumpsoverthelazydogandthefoxwasveryhappy'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = \"\"\n",
    "for i in sentence:\n",
    "    if(i != \" \"):\n",
    "        if(i.isupper()==True):\n",
    "            new_list += (i.lower())\n",
    "        if(i.islower()==True):\n",
    "            new_list += i\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list1 = list(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = []\n",
    "for i in new_list1:\n",
    "    list2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 1),\n",
       " ('h', 1),\n",
       " ('e', 1),\n",
       " ('q', 1),\n",
       " ('u', 1),\n",
       " ('i', 1),\n",
       " ('c', 1),\n",
       " ('k', 1),\n",
       " ('b', 1),\n",
       " ('r', 1),\n",
       " ('o', 1),\n",
       " ('w', 1),\n",
       " ('n', 1),\n",
       " ('f', 1),\n",
       " ('o', 1),\n",
       " ('x', 1),\n",
       " ('j', 1),\n",
       " ('u', 1),\n",
       " ('m', 1),\n",
       " ('p', 1),\n",
       " ('s', 1),\n",
       " ('o', 1),\n",
       " ('v', 1),\n",
       " ('e', 1),\n",
       " ('r', 1),\n",
       " ('t', 1),\n",
       " ('h', 1),\n",
       " ('e', 1),\n",
       " ('l', 1),\n",
       " ('a', 1),\n",
       " ('z', 1),\n",
       " ('y', 1),\n",
       " ('d', 1),\n",
       " ('o', 1),\n",
       " ('g', 1),\n",
       " ('a', 1),\n",
       " ('n', 1),\n",
       " ('d', 1),\n",
       " ('t', 1),\n",
       " ('h', 1),\n",
       " ('e', 1),\n",
       " ('f', 1),\n",
       " ('o', 1),\n",
       " ('x', 1),\n",
       " ('w', 1),\n",
       " ('a', 1),\n",
       " ('s', 1),\n",
       " ('v', 1),\n",
       " ('e', 1),\n",
       " ('r', 1),\n",
       " ('y', 1),\n",
       " ('h', 1),\n",
       " ('a', 1),\n",
       " ('p', 1),\n",
       " ('p', 1),\n",
       " ('y', 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_list = list(zip(new_list1, list2))\n",
    "tuple_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the list of tuples to a PySpark RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(tuple_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methods of PySpark RDD display the letter count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 3),\n",
       " ('h', 4),\n",
       " ('e', 5),\n",
       " ('q', 1),\n",
       " ('u', 2),\n",
       " ('i', 1),\n",
       " ('c', 1),\n",
       " ('k', 1),\n",
       " ('b', 1),\n",
       " ('r', 3),\n",
       " ('o', 5),\n",
       " ('w', 2),\n",
       " ('n', 2),\n",
       " ('f', 2),\n",
       " ('x', 2),\n",
       " ('j', 1),\n",
       " ('m', 1),\n",
       " ('p', 3),\n",
       " ('s', 2),\n",
       " ('v', 2),\n",
       " ('l', 1),\n",
       " ('a', 4),\n",
       " ('z', 1),\n",
       " ('y', 3),\n",
       " ('d', 2),\n",
       " ('g', 1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.reduceByKey(lambda x, y: x+y).collect()\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methods of PySpark RDD display the letter and number of times they appear in each\n",
    "word in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [('a', 1), ('a', 1), ('a', 1), ('a', 1)]),\n",
       " ('b', [('b', 1)]),\n",
       " ('c', [('c', 1)]),\n",
       " ('d', [('d', 1), ('d', 1)]),\n",
       " ('e', [('e', 1), ('e', 1), ('e', 1), ('e', 1), ('e', 1)]),\n",
       " ('f', [('f', 1), ('f', 1)]),\n",
       " ('g', [('g', 1)]),\n",
       " ('h', [('h', 1), ('h', 1), ('h', 1), ('h', 1)]),\n",
       " ('i', [('i', 1)]),\n",
       " ('j', [('j', 1)]),\n",
       " ('k', [('k', 1)]),\n",
       " ('l', [('l', 1)]),\n",
       " ('m', [('m', 1)]),\n",
       " ('n', [('n', 1), ('n', 1)]),\n",
       " ('o', [('o', 1), ('o', 1), ('o', 1), ('o', 1), ('o', 1)]),\n",
       " ('p', [('p', 1), ('p', 1), ('p', 1)]),\n",
       " ('q', [('q', 1)]),\n",
       " ('r', [('r', 1), ('r', 1), ('r', 1)]),\n",
       " ('s', [('s', 1), ('s', 1)]),\n",
       " ('t', [('t', 1), ('t', 1), ('t', 1)]),\n",
       " ('u', [('u', 1), ('u', 1)]),\n",
       " ('v', [('v', 1), ('v', 1)]),\n",
       " ('w', [('w', 1), ('w', 1)]),\n",
       " ('x', [('x', 1), ('x', 1)]),\n",
       " ('y', [('y', 1), ('y', 1), ('y', 1)]),\n",
       " ('z', [('z', 1)])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rdd.groupBy(lambda x: x[0]).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sql context from PySpark SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Amazon Review Dataset into a PySpark RDD, ensure that each row is properly\n",
    "separated and the headers are matched to their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'id', 'name', 'username'],\n",
       " ['0', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Cristina M'],\n",
       " ['1', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Ricky'],\n",
       " ['2', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Tedd Gardiner'],\n",
       " ['3', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Dougal'],\n",
       " ['4', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Miljan David Tanic'],\n",
       " ['5', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Kelvin Law'],\n",
       " ['6', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Ricky'],\n",
       " ['7', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Bandler'],\n",
       " ['8', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Cristina M']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sc.textFile(\"amazon_reviews.csv\")\n",
    "rdd2 = dataset.map(lambda line: line.split(\",\"))\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the rdd to a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rdd2)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataframe from the above question show the top 20 bought products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+--------------------+\n",
      "| _1|                  _2|               _3|                  _4|\n",
      "+---+--------------------+-----------------+--------------------+\n",
      "|   |                  id|             name|            username|\n",
      "|  0|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Cristina M|\n",
      "|  1|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|               Ricky|\n",
      "|  2|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|  3|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              Dougal|\n",
      "|  4|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|  Miljan David Tanic|\n",
      "|  5|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Kelvin Law|\n",
      "|  6|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|               Ricky|\n",
      "|  7|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|             Bandler|\n",
      "|  8|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Cristina M|\n",
      "|  9|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "| 10|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|     Miguel Martinez|\n",
      "| 11|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|   Magnus Brattemark|\n",
      "| 12|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "| 13|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|  Janet Matthews Jan|\n",
      "| 14|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|John Kat's the br...|\n",
      "| 15|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              samira|\n",
      "| 16|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|        Louis simard|\n",
      "| 17|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              JanetC|\n",
      "| 18|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|            Shepherd|\n",
      "+---+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|               _3|\n",
      "+-----------------+\n",
      "|             name|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "|Kindle Paperwhite|\n",
      "+-----------------+\n",
      "only showing top 21 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_3').show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the datafrme from the previous question show the top 20 users and the products that\n",
    "they purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|               _3|                  _4|\n",
      "+-----------------+--------------------+\n",
      "|             name|            username|\n",
      "|Kindle Paperwhite|          Cristina M|\n",
      "|Kindle Paperwhite|               Ricky|\n",
      "|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|Kindle Paperwhite|              Dougal|\n",
      "|Kindle Paperwhite|  Miljan David Tanic|\n",
      "|Kindle Paperwhite|          Kelvin Law|\n",
      "|Kindle Paperwhite|               Ricky|\n",
      "|Kindle Paperwhite|             Bandler|\n",
      "|Kindle Paperwhite|          Cristina M|\n",
      "|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|Kindle Paperwhite|     Miguel Martinez|\n",
      "|Kindle Paperwhite|   Magnus Brattemark|\n",
      "|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|Kindle Paperwhite|  Janet Matthews Jan|\n",
      "|Kindle Paperwhite|John Kat's the br...|\n",
      "|Kindle Paperwhite|              samira|\n",
      "|Kindle Paperwhite|        Louis simard|\n",
      "|Kindle Paperwhite|              JanetC|\n",
      "|Kindle Paperwhite|            Shepherd|\n",
      "|Kindle Paperwhite|              Brenda|\n",
      "+-----------------+--------------------+\n",
      "only showing top 21 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_3','_4').show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3', '_4']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a RDD of tuples from the dataframe with only 2 columns ‘product’ and ‘username’ in\n",
    "that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = df.rdd.map(lambda x: ROW(product = x[2], username = x[3]))\n",
    "type(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using methods from PySpark’s RDD object e.g. groupByKey, map, reduceByKey derive the top\n",
    "20 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.map(lambda x : {x[1]: list(x[0])}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another RDD of tuples from the dataframe with the columns ‘username’ and ‘product’\n",
    "in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data2 = df.rdd.map(lambda x: ROW(username = x[3], product = x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using methods form PySparks’s RDD object product the top 10 customers who purchased the\n",
    "most items. The top 10 list must show the username and a list of all the items they bought\n",
    "with the number of that item they bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = input_data2.groupBy(lambda x: x[0]).collect()\n",
    "rdd3 = sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd3.foldByKey(1, multiply).collect()\n",
    "rdd.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
